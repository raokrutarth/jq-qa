{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jd_qa_script_training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeS9EVSeTrl3",
        "outputId": "fcab92ab-c37f-44da-9f7b-7aaba4540653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 224 Âµs (started: 2022-05-30 09:43:40 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q datasets transformers\n",
        "!pip install -q ipython-autotime\n",
        "\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! mv /content/dev-v2.0.jsonl /content/dev-v2.0.json"
      ],
      "metadata": {
        "id": "unON-3cSVWtV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --max_train_samples 100\n",
        "\n",
        "!python ./transformers/examples/tensorflow/question-answering/run_qa.py \\\n",
        "    --output_dir ./my-finetuned-jd-qa-model \\\n",
        "    --model_name_or_path distilbert-base-uncased \\\n",
        "    --tokenizer_name distilbert-base-uncased \\\n",
        "    --train_file foo.csv \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --n_best_size 2 \\\n",
        "    --num_train_epochs 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpyAJhZTUW74",
        "outputId": "af14cbc2-276d-41e6-996a-62573ab03a83"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/30/2022 18:41:21 - INFO - __main__ - Training/evaluation parameters TFTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gcp_project=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./my-finetuned-jd-qa-model/runs/May30_18-41-21_c8143a326c42,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=./my-finetuned-jd-qa-model,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "poly_power=1.0,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./my-finetuned-jd-qa-model,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_name=None,\n",
            "tpu_num_cores=None,\n",
            "tpu_zone=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xla=False,\n",
            "xpu_backend=None,\n",
            ")\n",
            "05/30/2022 18:41:22 - WARNING - datasets.builder - Using custom data configuration default-de777f01bd7a9e0b\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-de777f01bd7a9e0b/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 6335.81it/s]\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 652.10it/s]\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-de777f01bd7a9e0b/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 342.56it/s]\n",
            "[INFO|configuration_utils.py:659] 2022-05-30 18:41:22,796 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
            "[INFO|configuration_utils.py:708] 2022-05-30 18:41:22,799 >> Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:659] 2022-05-30 18:41:23,358 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
            "[INFO|configuration_utils.py:708] 2022-05-30 18:41:23,359 >> Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-05-30 18:41:25,060 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-05-30 18:41:25,060 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-05-30 18:41:25,060 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-05-30 18:41:25,060 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1782] 2022-05-30 18:41:25,060 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|configuration_utils.py:659] 2022-05-30 18:41:25,338 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
            "[INFO|configuration_utils.py:708] 2022-05-30 18:41:25,340 >> Model config DistilBertConfig {\n",
            "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
            "  \"activation\": \"gelu\",\n",
            "  \"architectures\": [\n",
            "    \"DistilBertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.1,\n",
            "  \"dim\": 768,\n",
            "  \"dropout\": 0.1,\n",
            "  \"hidden_dim\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"distilbert\",\n",
            "  \"n_heads\": 12,\n",
            "  \"n_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"qa_dropout\": 0.1,\n",
            "  \"seq_classif_dropout\": 0.2,\n",
            "  \"sinusoidal_pos_embds\": false,\n",
            "  \"tie_weights_\": true,\n",
            "  \"transformers_version\": \"4.19.2\",\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|training_args_tf.py:189] 2022-05-30 18:41:25,412 >> Tensorflow: setting up strategy\n",
            "2022-05-30 18:41:27.475903: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "100% 4/4 [00:06<00:00,  1.52s/ba]\n",
            "100% 2/2 [00:27<00:00, 13.89s/ba]\n",
            "[INFO|modeling_tf_utils.py:1776] 2022-05-30 18:42:02,284 >> loading weights file my-finetuned-jd-qa-model/tf_model.h5\n",
            "2022-05-30 18:42:02.363506: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "[WARNING|modeling_tf_utils.py:1852] 2022-05-30 18:42:09,658 >> All model checkpoint layers were used when initializing TFDistilBertForQuestionAnswering.\n",
            "\n",
            "[WARNING|modeling_tf_utils.py:1862] 2022-05-30 18:42:09,658 >> All the layers of TFDistilBertForQuestionAnswering were initialized from the model checkpoint at my-finetuned-jd-qa-model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n",
            "[WARNING|modeling_tf_utils.py:911] 2022-05-30 18:42:09,658 >> No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour, please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n",
            "2022-05-30 18:42:09.988047: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
            "op: \"TensorSliceDataset\"\n",
            "input: \"Placeholder/_0\"\n",
            "attr {\n",
            "  key: \"Toutput_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"_cardinality\"\n",
            "  value {\n",
            "    i: 5752\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"is_files\"\n",
            "  value {\n",
            "    b: false\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"metadata\"\n",
            "  value {\n",
            "    s: \"\\n\\024TensorSliceDataset:0\"\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "Epoch 1/5\n",
            "719/719 [==============================] - ETA: 0s - loss: 1.9530[INFO|configuration_utils.py:446] 2022-05-30 18:44:12,318 >> Configuration saved in ./my-finetuned-jd-qa-model/config.json\n",
            "2022-05-30 18:44:12.340302: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 93763584 exceeds 10% of free system memory.\n",
            "[INFO|modeling_tf_utils.py:1507] 2022-05-30 18:44:15,610 >> Model weights saved in ./my-finetuned-jd-qa-model/tf_model.h5\n",
            "719/719 [==============================] - 125s 141ms/step - loss: 1.9530\n",
            "Epoch 2/5\n",
            "719/719 [==============================] - ETA: 0s - loss: 1.3690[INFO|configuration_utils.py:446] 2022-05-30 18:46:34,123 >> Configuration saved in ./my-finetuned-jd-qa-model/config.json\n",
            "2022-05-30 18:46:34.193628: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 93763584 exceeds 10% of free system memory.\n",
            "[INFO|modeling_tf_utils.py:1507] 2022-05-30 18:46:35,830 >> Model weights saved in ./my-finetuned-jd-qa-model/tf_model.h5\n",
            "719/719 [==============================] - 100s 139ms/step - loss: 1.3690\n",
            "Epoch 3/5\n",
            "719/719 [==============================] - ETA: 0s - loss: 1.1677[INFO|configuration_utils.py:446] 2022-05-30 18:48:56,118 >> Configuration saved in ./my-finetuned-jd-qa-model/config.json\n",
            "2022-05-30 18:48:56.197551: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 93763584 exceeds 10% of free system memory.\n",
            "[INFO|modeling_tf_utils.py:1507] 2022-05-30 18:48:57,427 >> Model weights saved in ./my-finetuned-jd-qa-model/tf_model.h5\n",
            "719/719 [==============================] - 100s 139ms/step - loss: 1.1677\n",
            "Epoch 4/5\n",
            "719/719 [==============================] - ETA: 0s - loss: 1.0137[INFO|configuration_utils.py:446] 2022-05-30 18:50:36,035 >> Configuration saved in ./my-finetuned-jd-qa-model/config.json\n",
            "2022-05-30 18:50:36.160187: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 93763584 exceeds 10% of free system memory.\n",
            "[INFO|modeling_tf_utils.py:1507] 2022-05-30 18:50:37,274 >> Model weights saved in ./my-finetuned-jd-qa-model/tf_model.h5\n",
            "719/719 [==============================] - 100s 139ms/step - loss: 1.0137\n",
            "Epoch 5/5\n",
            "719/719 [==============================] - ETA: 0s - loss: 0.9155[INFO|configuration_utils.py:446] 2022-05-30 18:52:57,680 >> Configuration saved in ./my-finetuned-jd-qa-model/config.json\n",
            "2022-05-30 18:52:57.790329: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 93763584 exceeds 10% of free system memory.\n",
            "[INFO|modeling_tf_utils.py:1507] 2022-05-30 18:52:59,001 >> Model weights saved in ./my-finetuned-jd-qa-model/tf_model.h5\n",
            "719/719 [==============================] - 99s 138ms/step - loss: 0.9155\n",
            "05/30/2022 18:52:59 - INFO - __main__ - *** Evaluation ***\n",
            "2022-05-30 18:53:05.507898: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
            "op: \"FlatMapDataset\"\n",
            "input: \"PrefetchDataset/_8\"\n",
            "attr {\n",
            "  key: \"Targuments\"\n",
            "  value {\n",
            "    list {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"_cardinality\"\n",
            "  value {\n",
            "    i: -2\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"f\"\n",
            "  value {\n",
            "    func {\n",
            "      name: \"__inference_Dataset_flat_map_slice_batch_indices_42323\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"metadata\"\n",
            "  value {\n",
            "    s: \"\\n\\021FlatMapDataset:36\"\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_shapes\"\n",
            "  value {\n",
            "    list {\n",
            "      shape {\n",
            "        dim {\n",
            "          size: -1\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "attr {\n",
            "  key: \"output_types\"\n",
            "  value {\n",
            "    list {\n",
            "      type: DT_INT64\n",
            "    }\n",
            "  }\n",
            "}\n",
            ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
            "100% 1270/1270 [00:08<00:00, 152.15it/s]\n",
            "Evaluation metrics:\n",
            "exact_match: 36.850\n",
            "f1: 40.578\n",
            "time: 12min 32s (started: 2022-05-30 18:41:04 +00:00)\n"
          ]
        }
      ]
    }
  ]
}